# ğŸš€ Quick Start - LLM API Gemma 3N TFLite

## âš¡ InÃ­cio RÃ¡pido

Este guia permite testar a API LLM em **menos de 5 minutos**!

## ğŸ“‹ PrÃ©-requisitos

- âœ… Sistema ATous rodando na porta 8000
- âœ… Modelo Gemma 3N TFLite carregado
- âœ… Postman ou similar instalado

## ğŸ”§ Passo 1: Verificar Status

### Health Check
```bash
GET http://127.0.0.1:8000/health
```

**Resposta esperada:**
```json
{
  "status": "healthy",
  "timestamp": "2025-08-17T22:15:00Z"
}
```

### Status do LLM
```bash
GET http://127.0.0.1:8000/api/llm/status
```

**Resposta esperada:**
```json
{
  "is_loaded": true,
  "model_type": "tflite",
  "model_path": "models/gemma-3n/extracted",
  "status": "ready"
}
```

## ğŸ¤– Passo 2: Primeira Consulta

### Consulta Simples
```bash
POST http://127.0.0.1:8000/api/llm/query
Content-Type: application/json

{
  "question": "Como estÃ¡ o sistema de seguranÃ§a?",
  "include_system_context": true
}
```

**Resposta esperada:**
```json
{
  "answer": "O sistema ATous Secure Network estÃ¡ funcionando normalmente com o modelo TFLite Gemma 3N.",
  "confidence": 0.8,
  "sources": ["llm", "system_metrics"],
  "metadata": {
    "question_type": "system_status",
    "response_length": 89,
    "has_context": true
  },
  "timestamp": "2025-08-17T22:15:00Z",
  "processing_time": 0.0008
}
```

## ğŸ“Š Passo 3: Verificar MÃ©tricas

### MÃ©tricas do LLM
```bash
GET http://127.0.0.1:8000/api/llm/metrics
```

**Resposta esperada:**
```json
{
  "total_queries": 1,
  "successful_responses": 1,
  "average_response_time": 0.0008,
  "cache_size": 1,
  "is_loaded": true,
  "model_type": "tflite"
}
```

## ğŸ¯ Passo 4: Testar Diferentes Tipos de Perguntas

### 1. Sistema e SeguranÃ§a
```json
{
  "question": "Qual Ã© o status atual do sistema de seguranÃ§a?"
}
```

### 2. DetecÃ§Ã£o de AmeaÃ§as
```json
{
  "question": "HÃ¡ alguma ameaÃ§a detectada no momento?",
  "include_system_context": true
}
```

### 3. UsuÃ¡rios e Acesso
```json
{
  "question": "Quantos usuÃ¡rios estÃ£o ativos?",
  "include_system_context": true
}
```

### 4. Componentes EspecÃ­ficos
```json
{
  "question": "Como funciona o sistema ABISS?",
  "context": {"component": "abiss"}
}
```

## ğŸ” Passo 5: Debugging

### Se algo nÃ£o funcionar:

1. **Verificar logs:**
   ```bash
   tail -f logs/atous_network.log
   ```

2. **Verificar se o modelo estÃ¡ carregado:**
   ```bash
   GET http://127.0.0.1:8000/api/llm/status
   ```

3. **Verificar se o servidor estÃ¡ rodando:**
   ```bash
   GET http://127.0.0.1:8000/health
   ```

4. **Verificar arquivo do modelo:**
   ```bash
   ls -la models/gemma-3n/extracted/
   ```

## ğŸ“± Usando Postman

### Importar ColeÃ§Ã£o
1. Abra o Postman
2. Clique em "Import"
3. Selecione o arquivo: `docs/api/ATous_LLM_API.postman_collection.json`
4. A coleÃ§Ã£o serÃ¡ importada com todos os endpoints prÃ©-configurados

### Testar Endpoints
1. **Health Check** - Verificar se o sistema estÃ¡ rodando
2. **LLM Status** - Verificar se o modelo estÃ¡ carregado
3. **LLM Query** - Fazer perguntas ao Gemma 3N TFLite
4. **LLM Metrics** - Ver mÃ©tricas de performance

## ğŸš¨ Problemas Comuns

### 1. "Connection refused"
- **SoluÃ§Ã£o**: Verificar se o servidor estÃ¡ rodando na porta 8000

### 2. "Modelo LLM nÃ£o estÃ¡ carregado"
- **SoluÃ§Ã£o**: Aguardar o carregamento ou verificar se o arquivo TFLite existe

### 3. "Timeout"
- **SoluÃ§Ã£o**: Aumentar timeout para 10-30 segundos

### 4. "500 Internal Server Error"
- **SoluÃ§Ã£o**: Verificar logs para detalhes do erro

## ğŸ‰ Sucesso!

Se vocÃª conseguiu:
- âœ… Fazer health check
- âœ… Ver status do LLM
- âœ… Fazer consultas
- âœ… Ver mÃ©tricas

**ParabÃ©ns! A API LLM estÃ¡ funcionando perfeitamente! ğŸ¯**

## ğŸ“š PrÃ³ximos Passos

- **DocumentaÃ§Ã£o Completa**: [LLM_API.md](LLM_API.md)
- **Swagger UI**: `http://127.0.0.1:8000/docs`
- **WebSocket**: Testar chat em tempo real
- **Casos de Uso**: Explorar diferentes tipos de perguntas

---

**ğŸ¤– Agora vocÃª pode conversar com o Gemma 3N TFLite via API!**
