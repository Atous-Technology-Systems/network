#!/usr/bin/env python3
"""
Script para debugar o carregamento do modelo LLM
"""

import os
import sys
from pathlib import Path

# Adicionar o projeto ao path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

def main():
    print("üîç Debugando carregamento do modelo LLM...")
    print("=" * 50)
    
    # 1. Verificar vari√°veis de ambiente
    print("\n1Ô∏è‚É£ Verificando vari√°veis de ambiente:")
    env_vars = [
        'GEMMA_MODEL_PATH',
        'MODEL_PATH', 
        'LLM_MODEL_PATH',
        'PYTHONPATH'
    ]
    
    for var in env_vars:
        value = os.getenv(var)
        if value:
            print(f"   {var}: {value}")
        else:
            print(f"   {var}: n√£o definida")
    
    # 2. Verificar diret√≥rio do modelo
    print("\n2Ô∏è‚É£ Verificando diret√≥rio do modelo:")
    model_dir = Path("models/gemma-3n-hf")
    if model_dir.exists():
        print(f"   ‚úÖ Diret√≥rio existe: {model_dir}")
        
        # Listar arquivos principais
        key_files = [
            'pytorch_model.bin',
            'config.json', 
            'tokenizer_config.json',
            'vocab.json'
        ]
        
        for file in key_files:
            file_path = model_dir / file
            if file_path.exists():
                size_mb = file_path.stat().st_size / (1024 * 1024)
                print(f"   ‚úÖ {file}: {size_mb:.1f} MB")
            else:
                print(f"   ‚ùå {file}: n√£o encontrado")
    else:
        print(f"   ‚ùå Diret√≥rio n√£o existe: {model_dir}")
    
    # 3. Verificar configura√ß√£o do LLM service
    print("\n3Ô∏è‚É£ Verificando configura√ß√£o do LLM service:")
    try:
        from atous_sec_network.ml.llm_service import LLMService
        
        # Criar inst√¢ncia para teste
        llm_service = LLMService()
        print(f"   Model path configurado: {llm_service.model_path}")
        print(f"   Model path absoluto: {Path(llm_service.model_path).absolute()}")
        print(f"   Model path existe: {Path(llm_service.model_path).exists()}")
        
        # Verificar se o caminho est√° correto
        expected_path = Path("models/gemma-3n-hf").absolute()
        actual_path = Path(llm_service.model_path).absolute()
        
        if expected_path == actual_path:
            print("   ‚úÖ Caminho do modelo est√° correto")
        else:
            print(f"   ‚ùå Caminho incorreto!")
            print(f"      Esperado: {expected_path}")
            print(f"      Atual:   {actual_path}")
            
    except Exception as e:
        print(f"   ‚ùå Erro ao verificar LLM service: {e}")
    
    # 4. Verificar se h√° conflitos de configura√ß√£o
    print("\n4Ô∏è‚É£ Verificando arquivos de configura√ß√£o:")
    config_files = [
        "atous_sec_network/config/gemma_3n_config.py",
        "atous_sec_network/config/gemma_config.py",
        ".env"
    ]
    
    for config_file in config_files:
        config_path = Path(config_file)
        if config_path.exists():
            print(f"   üìÑ {config_file}: existe")
            
            # Verificar se cont√©m refer√™ncias a DialoGPT
            try:
                with open(config_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                    if 'DialoGPT' in content:
                        print(f"      ‚ö†Ô∏è  Cont√©m refer√™ncias a DialoGPT")
                    if 'gemma-3n' in content.lower():
                        print(f"      ‚úÖ Cont√©m refer√™ncias a Gemma 3N")
            except Exception as e:
                print(f"      ‚ùå Erro ao ler: {e}")
        else:
            print(f"   ‚ùå {config_file}: n√£o existe")
    
    # 5. Testar carregamento do modelo
    print("\n5Ô∏è‚É£ Testando carregamento do modelo:")
    try:
        from transformers import AutoTokenizer, AutoModelForCausalLM
        
        model_path = str(model_dir.absolute())
        print(f"   Tentando carregar de: {model_path}")
        
        # Carregar tokenizer
        print("   Carregando tokenizer...")
        tokenizer = AutoTokenizer.from_pretrained(
            model_path,
            trust_remote_code=True,
            local_files_only=True
        )
        print(f"   ‚úÖ Tokenizer carregado: {type(tokenizer).__name__}")
        
        # Carregar modelo
        print("   Carregando modelo...")
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            torch_dtype="auto",
            device_map="auto",
            trust_remote_code=True,
            local_files_only=True
        )
        print(f"   ‚úÖ Modelo carregado: {type(model).__name__}")
        
        # Verificar informa√ß√µes do modelo
        print(f"   üìä Configura√ß√£o do modelo:")
        print(f"      Nome: {model.config.name_or_path}")
        print(f"      Tipo: {model.config.model_type}")
        print(f"      Vocab size: {model.config.vocab_size}")
        
    except Exception as e:
        print(f"   ‚ùå Erro ao carregar modelo: {e}")
        import traceback
        traceback.print_exc()
    
    print("\nüéØ Debug conclu√≠do!")

if __name__ == "__main__":
    main()
