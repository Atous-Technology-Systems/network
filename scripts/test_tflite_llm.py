#!/usr/bin/env python3
"""
Script para testar o LLM service com modelo TFLite
"""

import os
import sys
from pathlib import Path

# Adicionar o projeto ao path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

async def test_tflite_llm():
    """Testa o LLM service com modelo TFLite"""
    print("üß™ Testando LLM service com modelo TFLite...")
    print("=" * 50)
    
    try:
        from atous_sec_network.ml.llm_service import LLMService
        
        # Criar inst√¢ncia com caminho TFLite
        llm_service = LLMService("models/gemma-3n/extracted")
        print(f"‚úÖ LLM Service criado com caminho: {llm_service.model_path}")
        
        # Verificar se o diret√≥rio existe
        if not os.path.exists(llm_service.model_path):
            print(f"‚ùå Diret√≥rio do modelo n√£o existe: {llm_service.model_path}")
            return False
        
        print(f"‚úÖ Diret√≥rio do modelo existe")
        
        # Listar arquivos do modelo
        print("\nüìã Arquivos do modelo:")
        for item in Path(llm_service.model_path).glob("*"):
            if item.is_file():
                size_mb = item.stat().st_size / (1024 * 1024)
                print(f"   üìÑ {item.name}: {size_mb:.1f} MB")
        
        # Carregar modelo
        print("\nüîÑ Carregando modelo...")
        success = await llm_service.load_model()
        
        if success:
            print("‚úÖ Modelo carregado com sucesso!")
            print(f"   Tipo: {type(llm_service.model).__name__}")
            print(f"   Tokenizer: {type(llm_service.tokenizer).__name__}")
            print(f"   Carregado: {llm_service.is_loaded}")
            
            # Testar consulta
            print("\nüîç Testando consulta...")
            response = await llm_service.query("Como est√° o sistema de seguran√ßa?")
            print(f"   Resposta: {response.answer}")
            print(f"   Confian√ßa: {response.confidence}")
            print(f"   Fontes: {response.sources}")
            
            # Ver m√©tricas
            print("\nüìä M√©tricas:")
            metrics = llm_service.get_metrics()
            for key, value in metrics.items():
                print(f"   {key}: {value}")
            
        else:
            print("‚ùå Falha ao carregar modelo")
            return False
        
        return True
        
    except Exception as e:
        print(f"‚ùå Erro no teste: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """Fun√ß√£o principal"""
    import asyncio
    
    print("üöÄ Iniciando teste do LLM service TFLite...")
    
    # Executar teste ass√≠ncrono
    success = asyncio.run(test_tflite_llm())
    
    if success:
        print("\nüéâ Teste conclu√≠do com sucesso!")
    else:
        print("\n‚ùå Teste falhou!")
        sys.exit(1)

if __name__ == "__main__":
    main()
