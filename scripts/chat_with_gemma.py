#!/usr/bin/env python3
"""
Chat interativo com Gemma 3N TFLite
"""

import os
import sys
import asyncio
from pathlib import Path

# Adicionar o projeto ao path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

async def chat_with_gemma():
    """Chat interativo com Gemma 3N TFLite"""
    print("ğŸ¤– Chat com Gemma 3N TFLite - ATous Secure Network")
    print("=" * 60)
    print("ğŸ’¡ Digite 'sair' para encerrar o chat")
    print("ğŸ’¡ Digite 'ajuda' para ver comandos disponÃ­veis")
    print("ğŸ’¡ Digite 'status' para ver status do sistema")
    print("=" * 60)
    
    try:
        from atous_sec_network.ml.llm_service import LLMService
        
        # Inicializar LLM service
        print("\nğŸ”„ Inicializando Gemma 3N TFLite...")
        llm_service = LLMService("models/gemma-3n/extracted")
        
        # Carregar modelo
        success = await llm_service.load_model()
        if not success:
            print("âŒ Falha ao carregar modelo")
            return
        
        print("âœ… Gemma 3N TFLite carregado e pronto para conversar!")
        print(f"ğŸ“Š Modelo: {llm_service.get_metrics()['model_type']}")
        print(f"ğŸ¯ ConfianÃ§a base: 0.70-0.80")
        print()
        
        # Loop de chat
        conversation_history = []
        
        while True:
            try:
                # Input do usuÃ¡rio
                user_input = input("\nğŸ‘¤ VocÃª: ").strip()
                
                if not user_input:
                    continue
                
                # Comandos especiais
                if user_input.lower() == 'sair':
                    print("\nğŸ‘‹ AtÃ© logo! Gemma 3N TFLite encerrando...")
                    break
                
                elif user_input.lower() == 'ajuda':
                    print("\nğŸ“š Comandos disponÃ­veis:")
                    print("   sair     - Encerrar o chat")
                    print("   ajuda    - Mostrar esta ajuda")
                    print("   status   - Status do sistema")
                    print("   limpar   - Limpar histÃ³rico")
                    print("   mÃ©tricas - Ver mÃ©tricas do LLM")
                    print("   contexto - Ver contexto atual")
                    continue
                
                elif user_input.lower() == 'status':
                    print("\nğŸ“Š Status do Sistema:")
                    metrics = llm_service.get_metrics()
                    print(f"   Modelo: {metrics['model_type']}")
                    print(f"   Carregado: {metrics['is_loaded']}")
                    print(f"   Consultas: {metrics['total_queries']}")
                    print(f"   Cache: {metrics['cache_size']} entradas")
                    print(f"   Tempo mÃ©dio: {metrics['average_response_time']:.4f}s")
                    continue
                
                elif user_input.lower() == 'limpar':
                    conversation_history.clear()
                    print("\nğŸ§¹ HistÃ³rico de conversa limpo!")
                    continue
                
                elif user_input.lower() == 'mÃ©tricas':
                    print("\nğŸ“ˆ MÃ©tricas Detalhadas:")
                    metrics = llm_service.get_metrics()
                    for key, value in metrics.items():
                        print(f"   {key}: {value}")
                    continue
                
                elif user_input.lower() == 'contexto':
                    print("\nğŸŒ Contexto do Sistema:")
                    context = await llm_service.get_system_context()
                    for key, value in context.items():
                        if isinstance(value, dict):
                            print(f"   {key}:")
                            for sub_key, sub_value in value.items():
                                print(f"     {sub_key}: {sub_value}")
                        else:
                            print(f"   {key}: {value}")
                    continue
                
                # Processar pergunta
                print("\nğŸ¤– Gemma 3N TFLite estÃ¡ pensando...")
                
                # Adicionar ao histÃ³rico
                conversation_history.append({"role": "user", "content": user_input})
                
                # Obter resposta
                response = await llm_service.query(user_input)
                
                # Adicionar resposta ao histÃ³rico
                conversation_history.append({"role": "assistant", "content": response.answer})
                
                # Exibir resposta
                print(f"\nğŸ¤– Gemma 3N TFLite: {response.answer}")
                print(f"ğŸ¯ ConfianÃ§a: {response.confidence:.2f}")
                print(f"ğŸ“ Fontes: {', '.join(response.sources)}")
                
                # Manter histÃ³rico limitado
                if len(conversation_history) > 20:
                    conversation_history = conversation_history[-20:]
                
            except KeyboardInterrupt:
                print("\n\nğŸ‘‹ Chat interrompido pelo usuÃ¡rio. AtÃ© logo!")
                break
            except Exception as e:
                print(f"\nâŒ Erro: {e}")
                continue
        
        # EstatÃ­sticas finais
        print("\nğŸ“Š EstatÃ­sticas da Conversa:")
        metrics = llm_service.get_metrics()
        print(f"   Total de consultas: {metrics['total_queries']}")
        print(f"   Respostas bem-sucedidas: {metrics['successful_responses']}")
        print(f"   Tamanho do cache: {metrics['cache_size']}")
        print(f"   Tempo mÃ©dio de resposta: {metrics['average_response_time']:.4f}s")
        
    except Exception as e:
        print(f"\nâŒ Erro fatal: {e}")
        import traceback
        traceback.print_exc()

def main():
    """FunÃ§Ã£o principal"""
    print("ğŸš€ Iniciando chat com Gemma 3N TFLite...")
    
    # Executar chat assÃ­ncrono
    asyncio.run(chat_with_gemma())

if __name__ == "__main__":
    main()
