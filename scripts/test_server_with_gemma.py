#!/usr/bin/env python3
"""
Script para testar o servidor com integraÃ§Ã£o Gemma 3N TFLite
"""

import os
import sys
import asyncio
import requests
import time
from pathlib import Path

# Adicionar o projeto ao path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

async def test_server_integration():
    """Testa a integraÃ§Ã£o do servidor com Gemma 3N TFLite"""
    print("ğŸš€ Testando integraÃ§Ã£o do servidor com Gemma 3N TFLite...")
    print("=" * 60)
    
    # 1. Testar LLM service diretamente
    print("\n1ï¸âƒ£ Testando LLM service diretamente...")
    try:
        from atous_sec_network.ml.llm_service import LLMService
        
        llm_service = LLMService("models/gemma-3n/extracted")
        success = await llm_service.load_model()
        
        if success:
            print("   âœ… LLM service carregado com sucesso!")
            print(f"   ğŸ“Š Tipo do modelo: {llm_service.get_metrics()['model_type']}")
            
            # Testar consulta
            response = await llm_service.query("Qual Ã© o status do sistema de seguranÃ§a?")
            print(f"   ğŸ’¬ Resposta: {response.answer}")
            print(f"   ğŸ¯ ConfianÃ§a: {response.confidence}")
        else:
            print("   âŒ Falha ao carregar LLM service")
            return False
            
    except Exception as e:
        print(f"   âŒ Erro no LLM service: {e}")
        return False
    
    # 2. Testar servidor web
    print("\n2ï¸âƒ£ Testando servidor web...")
    try:
        import uvicorn
        from atous_sec_network.api.server import app
        
        print("   ğŸ”§ Iniciando servidor...")
        
        # Iniciar servidor em background
        config = uvicorn.Config(app, host="127.0.0.1", port=8000, log_level="info")
        server = uvicorn.Server(config)
        
        # Executar servidor em thread separada
        import threading
        server_thread = threading.Thread(target=server.run, daemon=True)
        server_thread.start()
        
        # Aguardar servidor inicializar
        print("   â³ Aguardando servidor inicializar...")
        await asyncio.sleep(5)
        
        # Testar endpoints
        base_url = "http://127.0.0.1:8000"
        
        # Health check
        try:
            response = requests.get(f"{base_url}/health", timeout=5)
            if response.status_code == 200:
                print("   âœ… Health check: OK")
            else:
                print(f"   âš ï¸  Health check: {response.status_code}")
        except Exception as e:
            print(f"   âŒ Health check falhou: {e}")
        
        # LLM metrics
        try:
            response = requests.get(f"{base_url}/api/llm/metrics", timeout=5)
            if response.status_code == 200:
                metrics = response.json()
                print("   âœ… LLM metrics: OK")
                print(f"      Modelo: {metrics.get('model_type', 'N/A')}")
                print(f"      Carregado: {metrics.get('is_loaded', False)}")
                print(f"      Consultas: {metrics.get('total_queries', 0)}")
            else:
                print(f"   âš ï¸  LLM metrics: {response.status_code}")
        except Exception as e:
            print(f"   âŒ LLM metrics falhou: {e}")
        
        # LLM query
        try:
            query_data = {"question": "Como estÃ¡ funcionando o sistema de seguranÃ§a?"}
            response = requests.post(f"{base_url}/api/llm/query", json=query_data, timeout=10)
            if response.status_code == 200:
                result = response.json()
                print("   âœ… LLM query: OK")
                print(f"      Resposta: {result.get('answer', 'N/A')[:100]}...")
                print(f"      ConfianÃ§a: {result.get('confidence', 0)}")
            else:
                print(f"   âš ï¸  LLM query: {response.status_code}")
                print(f"      Erro: {response.text}")
        except Exception as e:
            print(f"   âŒ LLM query falhou: {e}")
        
        print("   ğŸ‰ Servidor testado com sucesso!")
        
    except Exception as e:
        print(f"   âŒ Erro no servidor: {e}")
        return False
    
    return True

def main():
    """FunÃ§Ã£o principal"""
    print("ğŸ”§ Iniciando testes de integraÃ§Ã£o...")
    
    # Executar teste assÃ­ncrono
    success = asyncio.run(test_server_integration())
    
    if success:
        print("\nğŸ‰ Todos os testes passaram!")
        print("\nğŸ“ Resumo da integraÃ§Ã£o:")
        print("   âœ… LLM service com Gemma 3N TFLite funcionando")
        print("   âœ… Servidor web iniciado")
        print("   âœ… API endpoints respondendo")
        print("   âœ… Modelo TFLite carregado e respondendo")
        print("\nğŸŒ Servidor rodando em: http://127.0.0.1:8000")
        print("ğŸ” Teste a API em: http://127.0.0.1:8000/docs")
    else:
        print("\nâŒ Alguns testes falharam!")
        sys.exit(1)

if __name__ == "__main__":
    main()
